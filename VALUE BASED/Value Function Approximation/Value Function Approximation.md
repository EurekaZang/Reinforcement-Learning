ä¸‹é¢æ­£å¼çš„ä»‹ç»è¿™ä¸ªç®—æ³•ã€‚
æˆ‘ä»¬åˆšåˆšè¯´è¦ç”¨ä¸€ä¸ªæ›²çº¿æ¥å‚¨å­˜æ‰€æœ‰çš„state valueï¼Œä¹Ÿå°±æ˜¯è¿™å…¶å®æ˜¯åœ¨åšpolicy evaluationï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¯•å›¾æ±‚è§£æœ€ä¼˜çš„state value $v_\pi(s)$ã€‚
æˆ‘ä»¬åˆšåˆšåˆè¯´äº†ï¼Œç”±äºfeature vector $\phi^T(s)$ æ˜¯ä¸å˜çš„ï¼ˆç»™å®šçš„ï¼‰ï¼Œæˆ‘ä»¬çš„å€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•åªéœ€è¦æ›´æ–°parameter vector $w$ã€‚ç›®æ ‡å°±æ˜¯ä¼˜åŒ–ä¸€ä¸ªæ–¹ç¨‹ã€‚
åˆ†ä¸ºä¸¤æ­¥ï¼š
1. æ­£å¼çš„å®šä¹‰è¿™ä¸ªç›®æ ‡æ–¹ç¨‹ï¼ˆä¼˜åŒ–ç›®æ ‡ï¼‰
2. è®¨è®ºä¼˜åŒ–æ–¹æ³•
é¦–å…ˆæ¥çœ‹è¿™ä¸ªç›®æ ‡å‡½æ•°ï¼š$$J(w) = \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2]$$è¿™å°±æ˜¯å¤§åé¼é¼çš„å‡æ–¹è¯¯å·® **Mean Squared Error, MSE**ï¼Œå°±æ˜¯çœŸå®å€¼å’Œé¢„æµ‹å€¼çš„å¹³æ–¹è¯¯å·®çš„å‡å€¼ã€‚
ä¸€èˆ¬æƒ…å†µä¸‹å‡æ–¹è¯¯å·®å±•å¼€æ˜¯é•¿è¿™æ ·ï¼š$$J(w) = \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2] = \frac{1}{|S|} \sum_{s \in S} (v_\pi(s) - \hat{v}(s, w))^2$$ä½†æ˜¯è¿™æ ·é»˜è®¤æ‰€æœ‰åœ¨state space $\mathcal{S}$ ä¸­çš„æ‰€æœ‰stateçš„æƒé‡æ˜¯ç›¸åŒçš„ã€‚è¿™å¾ˆæ˜æ˜¾æ˜¯ä¸å¯¹çš„ï¼Œè®¾æƒ³agentä»ä¸€ä¸ªstateå‡ºå‘è¦åˆ°è¾¾target stateï¼Œç¦»target stateè¿‘çš„stateæ˜æ˜¾æ¯”ç¦»target stateå¾ˆè¿œçš„stateæ›´é‡è¦ã€‚ä¸ºäº†ä½“ç°é‡è¦æ€§ï¼Œæˆ‘ä»¬è¦ç»™ä»–ä»¬ä¸åŒçš„æƒé‡ã€‚ä¹Ÿå°±æ˜¯æ”¹å†™è¿™ä¸ªprobability distributionã€‚
é‚£æˆ‘ä»¬æ€ä¹ˆçŸ¥é“è¿™äº›stateçš„é‡è¦æ€§å‘¢ï¼Ÿè¿™é‡Œå¼•å…¥ä¸€ä¸ªlong-runæ€æƒ³ã€‚ä¹Ÿå°±æ˜¯è®©agentåœ¨environmentä¸­è¿›è¡Œé©¬å°”å¯å¤«è¡Œä¸ºï¼Œå½“episodeå¾ˆå¤§çš„æ—¶å€™æ¦‚ç‡åˆ†å¸ƒä¼šè¶‹äºç¨³å®šï¼Œè¿™ä¸ªç¨³å®šçš„æ¦‚ç‡åˆ†å¸ƒæˆ‘ä»¬å«åš***stationary distribution***ä¹Ÿå°±æ˜¯***é™æ€åˆ†å¸ƒ***ã€‚ä»stationary distributionä¸­å¯ä»¥çŸ¥é“å“ªäº›stateè¢«è®¿é—®çš„æ¬¡æ•°/é¢‘ç‡æœ€é«˜ã€‚è¶Šé«˜çš„æ¬¡æ•°/é¢‘ç‡å°±ä»£è¡¨ç€åº”å¾—çš„æ›´é«˜çš„æƒé‡ã€‚
æ•°å­¦å®šä¹‰ï¼š
Let $\{d_\pi(s)\}_{s \in S}$ denote the stationary distribution of the Markov process under policy $\pi$. By definition, $d_\pi(s) \geq 0$ and $\sum_{s \in S} d_\pi(s) = 1$.
é‚£ä¹ˆç›®æ ‡å‡½æ•°å°±å¯ä»¥å±•å¼€æˆè¿™æ ·ï¼š$$J(w) = \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2] = \sum_{s \in S} d_\pi(s) (v_\pi(s) - \hat{v}(s, w))^2$$
ç”¨ä¸€ä¸ªä¾‹å­æ¥æ›´å¥½çš„ç†è§£stationary distributionï¼š![[Pasted image 20250218114111.png]]æˆ‘ä»¬è®©agentåœ¨å¦‚å›¾çš„ç½‘æ ¼ä¸–ç•Œï¼ˆå›ºå®šç­–ç•¥ $\pi$ï¼‰ä¸­è·‘å¾ˆå¤šä¸ªepisodeï¼Œç„¶åç»™å‡ºPercentage each state visitedè®¡ç®—æ–¹å¼å¦‚ä¸‹$$d_\pi(s) \approx \frac{n_\pi(s)}{\sum_{s' \in S} n_\pi(s')}$$ä¹Ÿå°±æ˜¯$$\frac{\text{è¯¥stateçš„è®¿é—®æ¬¡æ•°}}{æ€»æ­¥æ•°}$$
å›åˆ°åˆšåˆšçš„ç›®æ ‡å‡½æ•°
åªè¦minimizeè¿™ä¸ªå‡æ–¹è¯¯å·®ï¼Œæˆ‘ä»¬å°±èƒ½å®ç°ä¼˜åŒ– $w$ çš„ç›®æ ‡ã€‚å› ä¸ºé¢„æµ‹å€¼å‘çœŸå®å€¼é€æ¸é è¿‘ã€‚
è¯´ç™½äº†å°±æ˜¯æœ€å°åŒ–è¿™ä¸ªæ–¹ç¨‹ã€‚æåˆ°æœ€å°åŒ–ï¼Œæˆ‘ä»¬é¦–å…ˆæƒ³åˆ°çš„å°±æ˜¯æ¢¯åº¦ä¸‹é™ GD ç®—æ³•ã€‚$$w_{k+1} = w_k - \alpha_k \nabla_w J(w_k)$$åŒ–ç®€ä¸€ä¸‹ï¼š$$\begin{aligned}
\nabla_w J(w) &= \nabla_w \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2] \\
&= \mathbb{E}[\nabla_w (v_\pi(S) - \hat{v}(S, w))^2] \\
&= 2\mathbb{E}[(v_\pi(S) - \hat{v}(S, w))(-\nabla_w \hat{v}(S, w))] \\
&= -2\mathbb{E}[(v_\pi(S) - \hat{v}(S, w))\nabla_w \hat{v}(S, w)]
\end{aligned}$$å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰ä¸ªæœŸæœ›ï¼Œé‚£ä¸å¦‚ç›´æ¥ä¸ŠSGDï¼š$$w_{t+1} = w_t + \alpha_t (v_\pi(s_t) - \hat{v}(s_t, w_t)) \nabla_w \hat{v}(s_t, w_t),$$å…¶ä¸­ $2\alpha_t$ ç®€å†™æˆäº† $\alpha_t$ ã€‚å°†æ‰€æœ‰çš„æœŸæœ›éƒ½æ¢æˆäº†éšæœºé‡‡æ ·ã€‚æ¬¸ä½†æ˜¯æˆ‘ä»¬ğŸ‘‰å‘ç°ï¼Œæ–¹ç¨‹ä¸­æœ‰ä¸€ä¸ª $v_\pi(s_t)$ è¿™ä¸ªæˆ‘ä»¬ä¸çŸ¥é“ï¼Œç”šè‡³å°±æ˜¯æˆ‘ä»¬è¦æ±‚çš„ã€‚æ€ä¹ˆåŠï¼Ÿé¦–å…ˆå¯ä»¥ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•ï¼Œåœ¨å½“å‰stateå¼€å¯å¾ˆå¤šä¸ªepisodeï¼Œç„¶åå¯¹ä»–ä»¬çš„returnæ±‚å‡å€¼ï¼š$$w_{t+1} = w_t + \alpha_t (g_t - \hat{v}(s_t, w_t))\nabla_w\hat{v}(s_t, w_t).$$æ—¢ç„¶å¯ä»¥ç”¨MCï¼Œé‚£ä¹ˆTDä¹Ÿè‡ªç„¶èƒ½ç”¨ï¼š$$w_{t+1} = w_t + \alpha_t \overbrace{\underbrace{[r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t)}_{\text{TD target}} - \hat{v}(s_t, w_t)]}^{\text{TD error}} \nabla_w \hat{v}(s_t, w_t).$$
ä¼ªä»£ç ï¼š
**Initialization:** A function $\hat{v}(s, w)$ that is a differentiable in $w$. Initial parameter $w_0$.
**Aim:** Approximate the true state values of a given policy $\pi$.
For each episode generated following the policy $\pi$, do
  For each step $(s_t, r_{t+1}, s_{t+1})$, do
    In the general case,
    $w_{t+1} = w_t + \alpha_t [r_{t+1} + \gamma \hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t)] \nabla_w \hat{v}(s_t, w_t)$
    In the linear case,
    $w_{t+1} = w_t + \alpha_t [r_{t+1} + \gamma \phi^T(s_{t+1})w_t - \phi^T(s_t)w_t]\phi(s_t)$

ä¸‹é¢æ¥ä»‹ç»å¦‚ä½•é€‰å–so-called $\hat{v}(s_t,w_t)$ï¼Œæœ‰ä¸¤ç§ä¸»æµæ€è·¯ï¼š
1. ä½¿ç”¨çº¿æ€§å‡½æ•°ï¼š$$\hat{v}(s_t,w_t)=\phi^T(s)w$$ä¹Ÿå°±æ˜¯feature vectorä¹˜parameter vectorï¼Œæ˜¯ä¸€ä¸ªçº¿æ€§çš„å…³ç³»ã€‚
2. ä½¿ç”¨ç¥ç»ç½‘ç»œï¼š$$\hat{v}(s_t,w_t)=f^w(s)$$è¿™é‡Œçš„ç¥ç»ç½‘ç»œå°±æ˜¯ä¸€ä¸ªéçº¿æ€§çš„å‡½æ•°ã€‚

å¦‚æœé€‰æ‹©çº¿æ€§å‡½æ•°ï¼Œé‚£ä¹ˆï¼š$$\nabla_w \hat{v}(s, w) = \phi(s).$$å¯ä»¥ç›´æ¥å°† $\phi(s)$ å¸¦å…¥TDæ–¹ç¨‹ä¸­ã€‚ä¹Ÿå°±è¯ç”Ÿäº†TD-Linearã€‚

æœ€åè¿˜æœ‰ä¸€ä¸ªå°ç‰¹æ€§ï¼Œé‚£å°±æ˜¯Tabularå½¢å¼çš„TDç®—æ³•å¯ä»¥å’Œå€¼å‡½æ•°è¿‘ä¼¼çš„ç®—æ³•è¿›è¡Œç»Ÿä¸€ã€‚æ€ä¹ˆåšåˆ°å‘¢ï¼Ÿé‚£å°±æ˜¯é€‰æ‹©çº¿æ€§ï¼Œç„¶åè®©feature vectorå˜æˆä¸€ä¸ªç‹¬çƒ­å‘é‡ã€‚è¿™é‡Œå°±ä¸å±•å¼€äº†ã€‚

 